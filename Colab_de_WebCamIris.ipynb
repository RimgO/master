{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Colab_de_WebCamIris.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPE2nYoFelTHvAHX9yanInZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RimgO/master/blob/main/Colab_de_WebCamIris.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget   http://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2\n",
        "!bunzip2 /content/shape_predictor_68_face_landmarks.dat.bz2\n",
        "!rm /content/shape_predictor_68_face_landmarks.dat.bz2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Kk2t1AyBI8L",
        "outputId": "ac857687-ecf0-4bf3-bb0c-8a22e5c54309"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-12-27 15:57:31--  http://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2\n",
            "Resolving dlib.net (dlib.net)... 107.180.26.78\n",
            "Connecting to dlib.net (dlib.net)|107.180.26.78|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 64040097 (61M)\n",
            "Saving to: ‘shape_predictor_68_face_landmarks.dat.bz2’\n",
            "\n",
            "shape_predictor_68_ 100%[===================>]  61.07M  23.9MB/s    in 2.6s    \n",
            "\n",
            "2021-12-27 15:57:34 (23.9 MB/s) - ‘shape_predictor_68_face_landmarks.dat.bz2’ saved [64040097/64040097]\n",
            "\n",
            "rm: cannot remove '/content/shape_predictor_68_face_landmarks.dat.bz2': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "ZlZ9ZAh8W-QG"
      },
      "outputs": [],
      "source": [
        "# import dependencies\n",
        "from IPython.display import display, Javascript, Image\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode, b64encode\n",
        "import cv2\n",
        "import dlib\n",
        "import numpy as np\n",
        "import PIL\n",
        "import io\n",
        "import html\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# function to convert the JavaScript object into an OpenCV image\n",
        "def js_to_image(js_reply):\n",
        "  \"\"\"\n",
        "  Params:\n",
        "          js_reply: JavaScript object containing image from webcam\n",
        "  Returns:\n",
        "          img: OpenCV BGR image\n",
        "  \"\"\"\n",
        "  # decode base64 image\n",
        "  image_bytes = b64decode(js_reply.split(',')[1])\n",
        "  # convert bytes to numpy array\n",
        "  jpg_as_np = np.frombuffer(image_bytes, dtype=np.uint8)\n",
        "  # decode numpy array into OpenCV BGR image\n",
        "  img = cv2.imdecode(jpg_as_np, flags=1)\n",
        "\n",
        "  return img\n",
        "\n",
        "# function to convert OpenCV Rectangle bounding box image into base64 byte string to be overlayed on video stream\n",
        "def bbox_to_bytes(bbox_array):\n",
        "  \"\"\"\n",
        "  Params:\n",
        "          bbox_array: Numpy array (pixels) containing rectangle to overlay on video stream.\n",
        "  Returns:\n",
        "        bytes: Base64 image byte string\n",
        "  \"\"\"\n",
        "  # convert array into PIL image\n",
        "  bbox_PIL = PIL.Image.fromarray(bbox_array, 'RGBA')\n",
        "  iobuf = io.BytesIO()\n",
        "  # format bbox into png for return\n",
        "  bbox_PIL.save(iobuf, format='png')\n",
        "  # format return string\n",
        "  bbox_bytes = 'data:image/png;base64,{}'.format((str(b64encode(iobuf.getvalue()), 'utf-8')))\n",
        "\n",
        "  return bbox_bytes\n",
        "\n",
        "  # function to convert OpenCV Rectangle bounding box image into base64 byte string to be overlayed on video stream\n",
        "def circle_to_bytes(circle_array):\n",
        "  \"\"\"\n",
        "  Params:\n",
        "          circle_array: Numpy array (pixels) containing circle to overlay on video stream.\n",
        "  Returns:\n",
        "        bytes: Base64 image byte string\n",
        "  \"\"\"\n",
        "  # convert array into PIL image\n",
        "  circle_PIL = PIL.Image.fromarray(circle_array, 'RGBA')\n",
        "  iobuf = io.BytesIO()\n",
        "  # format bbox into png for return\n",
        "  circle_PIL.save(iobuf, format='png')\n",
        "  # format return string\n",
        "  circle_bytes = 'data:image/png;base64,{}'.format((str(b64encode(iobuf.getvalue()), 'utf-8')))\n",
        "\n",
        "  return circle_bytes"
      ],
      "metadata": {
        "id": "lPFSOS37XFoN"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# coding:utf-8\n",
        "\n",
        "import os\n",
        "import dlib\n",
        "from imutils import face_utils\n",
        "import cv2\n",
        "\n",
        "\n",
        "class FaceLandmarkManager:\n",
        "    \"\"\"\n",
        "    顔のランドマーク管理クラス\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        顔のランドマーク検出ツールの呼び出し\n",
        "        \"\"\"\n",
        "        self._face_detector = dlib.get_frontal_face_detector()\n",
        "        ##print('current_path:',os.getcwd())\n",
        "        predictor_path = \"/content\" + os.sep + 'shape_predictor_68_face_landmarks.dat'\n",
        "        self._face_predictor = dlib.shape_predictor(predictor_path)\n",
        "\n",
        "        # 顔のランドマークリストの初期化\n",
        "        self._face_landmark_list = []\n",
        "        self._faces = []\n",
        "\n",
        "    def clear_face_landmark_list(self):\n",
        "        \"\"\"\n",
        "        顔のランドマークリストをリセット\n",
        "        \"\"\"\n",
        "        self._face_landmark_list = []\n",
        "\n",
        "    def detect_face_landmark(self, img):\n",
        "        \"\"\"\n",
        "        画像から顔のランドマークを取得し保存\n",
        "        :param img: 入力画像\n",
        "        \"\"\"\n",
        "\n",
        "        # 顔検出\n",
        "        img_gry = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "        self._faces = self._face_detector(img_gry, 1)\n",
        "\n",
        "        # 検出した全顔に対して処理\n",
        "        for face in self._faces:\n",
        "            # 顔のランドマーク検出\n",
        "            landmark = self._face_predictor(img_gry, face)\n",
        "            # 処理高速化のためランドマーク群をNumPy配列に変換(必須)\n",
        "            landmark = face_utils.shape_to_np(landmark)\n",
        "            self._face_landmark_list.append(landmark)\n",
        "\n",
        "\n",
        "    def draw_face_landmark_list(self, img):\n",
        "        \"\"\"\n",
        "        顔のランドマークを画像に描画\n",
        "        :param img: 描画対象の画像\n",
        "        \"\"\"\n",
        "\n",
        "        # 検出した顔全てにランドマークを描画\n",
        "        for landmark in self._face_landmark_list:\n",
        "            for (x, y) in landmark:\n",
        "                cv2.circle(img, (x, y), 1, (255, 1, 1), -1)\n",
        "\n",
        "    def get_face_landmark_list(self):\n",
        "        \"\"\"\n",
        "        顔のランドマークリストの受け渡し\n",
        "        :return self._face_landmark_list: 顔のランドマークリスト\n",
        "        \"\"\"\n",
        "        return self._face_landmark_list\n"
      ],
      "metadata": {
        "id": "qLjss6ixCYGI"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pタイル法用：眼球画像における虹彩の割合\n",
        "IRIS_PER = 0.4\n",
        "\n",
        "\n",
        "class EyeSystemManager:\n",
        "    \"\"\"\n",
        "    目関連の処理の管理クラス\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        # 目のランドマークの初期化\n",
        "        self._eye_info = None\n",
        "\n",
        "    def detect_eye_region(self, face_landmark):\n",
        "        \"\"\"\n",
        "        顔のランドマークから目の領域を取得\n",
        "        :param face_landmark:\n",
        "        \"\"\"\n",
        "        # 顔のランドマークから両目の領域を格納後、リストへ追加\n",
        "        eye_info = EyeRegionManager()\n",
        "        eye_info.detect_eye_region(face_landmark)\n",
        "\n",
        "        self._eye_info = eye_info\n",
        "\n",
        "    @staticmethod\n",
        "    def _detect_iris(eye_img):\n",
        "        # グレースケール化後、ガウシアンフィルタによる平滑化\n",
        "        eye_img_gry = cv2.cvtColor(eye_img, cv2.COLOR_BGR2GRAY)\n",
        "        eye_img_gau = cv2.GaussianBlur(eye_img_gry, (5, 5), 0)\n",
        "\n",
        "        # Pタイル法による2値化\n",
        "        eye_img_thr = p_tile_threshold(eye_img_gau, IRIS_PER)\n",
        "\n",
        "        cv2.rectangle(eye_img_thr, (0, 0), (eye_img_thr.shape[1] - 1, eye_img_thr.shape[0] - 1), (255, 255, 255), 1)\n",
        "\n",
        "        # 輪郭抽出\n",
        "        contours, hierarchy = cv2.findContours(eye_img_thr, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "        # 輪郭から最小外接円により虹彩を求める\n",
        "        iris = {'center': (0, 0), 'radius': 0}\n",
        "        for i, cnt in enumerate(contours):\n",
        "            (x, y), radius = cv2.minEnclosingCircle(cnt)\n",
        "            center = (int(x), int(y))\n",
        "            radius = int(radius)\n",
        "\n",
        "            # 半径が大きすぎる場合、虹彩候補から除外\n",
        "            if radius*0.3 < eye_img_thr.shape[0] < radius*0.8 :\n",
        "                # # 虹彩候補の描画\n",
        "                # cv2.circle(eye_img, center, radius, (255, 0, 0))\n",
        "                continue\n",
        "\n",
        "            # 最も半径が大きい円を虹彩と認定\n",
        "            if iris['radius'] < radius:\n",
        "                iris['center'] = center\n",
        "                iris['radius'] = radius\n",
        "                iris['num'] = i\n",
        "\n",
        "        return iris\n",
        "\n",
        "    def detect_iris_info(self, img):\n",
        "        \"\"\"\n",
        "        両目の虹彩の取得\n",
        "        :param img:\n",
        "        :return:\n",
        "        \"\"\"\n",
        "\n",
        "        # 眼球画像の取得\n",
        "        self._eye_info.detect_eye_img(img)\n",
        "        right_eye_img = self._eye_info.get_right_eye_img()\n",
        "        left_eye_img = self._eye_info.get_left_eye_img()\n",
        "\n",
        "        # 眼球画像から虹彩を抽出\n",
        "        right_iris = self._detect_iris(right_eye_img)\n",
        "        left_iris = self._detect_iris(left_eye_img)\n",
        "\n",
        "        # 元画像における眼球座標と、眼球画像からの相対的な虹彩座標から、\n",
        "        # 元画像における虹彩座標を計算\n",
        "        right_eye_region = self._eye_info.get_right_eye_region()\n",
        "        left_eye_region = self._eye_info.get_left_eye_region()\n",
        "\n",
        "        right_center = (int(right_iris['center'][0] + right_eye_region['top_x']),\n",
        "                        int(right_iris['center'][1] + right_eye_region['top_y']))\n",
        "        left_center = (int(left_iris['center'][0] + left_eye_region['top_x']),\n",
        "                       int(left_iris['center'][1] + left_eye_region['top_y']))\n",
        "\n",
        "        right_iris['center'] = right_center\n",
        "        left_iris['center'] = left_center\n",
        "\n",
        "        return right_iris, left_iris\n",
        "\n",
        "    def get_eye_region(self):\n",
        "        \"\"\"\n",
        "        目領域の受け渡し\n",
        "        :return self._eye_region: 目領域\n",
        "        \"\"\"\n",
        "        return self._eye_info"
      ],
      "metadata": {
        "id": "dhlElc8iDYB2"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EyeRegionManager:\n",
        "    \"\"\"\n",
        "    目領域を管理するクラス\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        初期化\n",
        "        \"\"\"\n",
        "        self._right_eye_region = {}\n",
        "        self._left_eye_region = {}\n",
        "        self._right_eye_img = None\n",
        "        self._left_eye_img = None\n",
        "        self._right_eye_points = None\n",
        "        self._left_eye_points = None\n",
        "\n",
        "    def detect_eye_region(self, face_landmark):\n",
        "        \"\"\"\n",
        "        ランドマークから目領域の座標を取得\n",
        "        :param face_landmark:\n",
        "        \"\"\"\n",
        "        # 右目切り出し\n",
        "        self._right_eye_region = {'top_x': face_landmark[36][0], 'bottom_x': face_landmark[39][0],\n",
        "                                  'top_y': face_landmark[37][1]\n",
        "                                  if face_landmark[37][1] < face_landmark[38][1] else face_landmark[38][1],\n",
        "                                  'bottom_y': face_landmark[41][1]\n",
        "                                  if face_landmark[41][1] > face_landmark[40][1] else face_landmark[40][1]}\n",
        "\n",
        "        # 左目切り出し\n",
        "        self._left_eye_region = {'top_x': face_landmark[42][0], 'bottom_x': face_landmark[45][0],\n",
        "                                 'top_y': face_landmark[43][1]\n",
        "                                 if face_landmark[43][1] < face_landmark[45][1] else face_landmark[45][1],\n",
        "                                 'bottom_y': face_landmark[47][1]\n",
        "                                 if face_landmark[47][1] > face_landmark[46][1] else face_landmark[46][1]}\n",
        "\n",
        "    def detect_eye_img(self, img):\n",
        "        \"\"\"\n",
        "        目領域の座標から左右の目画像を取得\n",
        "        :param img: 画像\n",
        "        \"\"\"\n",
        "        self._right_eye_img = img[self._right_eye_region['top_y']:self._right_eye_region['bottom_y'],\n",
        "                                  self._right_eye_region['top_x']:self._right_eye_region['bottom_x']]\n",
        "\n",
        "        self._left_eye_img = img[self._left_eye_region['top_y']:self._left_eye_region['bottom_y'],\n",
        "                                 self._left_eye_region['top_x']:self._left_eye_region['bottom_x']]\n",
        "\n",
        "    def get_right_eye_region(self):\n",
        "        \"\"\"\n",
        "        右目領域を受け渡す\n",
        "        :return self._right_eye_region: 右目領域\n",
        "        \"\"\"\n",
        "        return self._right_eye_region\n",
        "\n",
        "    def get_left_eye_region(self):\n",
        "        \"\"\"\n",
        "        左目領域を受け渡す\n",
        "        :return self._left_eye_region: 左目領域\n",
        "        \"\"\"\n",
        "        return self._left_eye_region\n",
        "\n",
        "    def get_right_eye_img(self):\n",
        "        \"\"\"\n",
        "        右目画像を受け渡す\n",
        "        :return self._right_eye_img: 右目画像\n",
        "        \"\"\"\n",
        "        return self._right_eye_img\n",
        "\n",
        "    def get_left_eye_img(self):\n",
        "        \"\"\"\n",
        "        左目画像を受け渡す\n",
        "        :return self._left_eye_img: 左目画像\n",
        "        \"\"\"\n",
        "        return self._left_eye_img"
      ],
      "metadata": {
        "id": "nskA9qL7JJ1f"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def p_tile_threshold(img_gry, per):\n",
        "    \"\"\"\n",
        "    Pタイル法による2値化処理\n",
        "    :param img_gry: 2値化対象のグレースケール画像\n",
        "    :param per: 2値化対象が画像で占める割合\n",
        "    :return img_thr: 2値化した画像\n",
        "    \"\"\"\n",
        "\n",
        "    # ヒストグラム取得\n",
        "    img_hist = cv2.calcHist([img_gry], [0], None, [256], [0, 256])\n",
        "\n",
        "    # 2値化対象が画像で占める割合から画素数を計算\n",
        "    all_pic = img_gry.shape[0] * img_gry.shape[1]\n",
        "    pic_per = all_pic * per\n",
        "\n",
        "    # Pタイル法による2値化のしきい値計算\n",
        "    p_tile_thr = 0\n",
        "    pic_sum = 0\n",
        "\n",
        "    # 現在の輝度と輝度の合計(高い値順に足す)の計算\n",
        "    for hist in img_hist:\n",
        "        pic_sum += hist\n",
        "\n",
        "        # 輝度の合計が定めた割合を超えた場合処理終了\n",
        "        if pic_sum > pic_per:\n",
        "            break\n",
        "\n",
        "        p_tile_thr += 1\n",
        "\n",
        "    # Pタイル法によって取得したしきい値で2値化処理\n",
        "    ret, img_thr = cv2.threshold(img_gry, p_tile_thr, 255, cv2.THRESH_BINARY)\n",
        "\n",
        "    return img_thr"
      ],
      "metadata": {
        "id": "ae1phsHMJq_G"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# JavaScript to properly create our live video stream using our webcam as input\n",
        "def video_stream():\n",
        "  js = Javascript('''\n",
        "    var video;\n",
        "    var div = null;\n",
        "    var stream;\n",
        "    var captureCanvas;\n",
        "    var imgElement;\n",
        "    var labelElement;\n",
        "    \n",
        "    var pendingResolve = null;\n",
        "    var shutdown = false;\n",
        "    \n",
        "    function removeDom() {\n",
        "       stream.getVideoTracks()[0].stop();\n",
        "       video.remove();\n",
        "       div.remove();\n",
        "       video = null;\n",
        "       div = null;\n",
        "       stream = null;\n",
        "       imgElement = null;\n",
        "       captureCanvas = null;\n",
        "       labelElement = null;\n",
        "    }\n",
        "    \n",
        "    function onAnimationFrame() {\n",
        "      if (!shutdown) {\n",
        "        window.requestAnimationFrame(onAnimationFrame);\n",
        "      }\n",
        "      if (pendingResolve) {\n",
        "        var result = \"\";\n",
        "        if (!shutdown) {\n",
        "          captureCanvas.getContext('2d').drawImage(video, 0, 0, 640, 480);\n",
        "          result = captureCanvas.toDataURL('image/jpeg', 0.8)\n",
        "        }\n",
        "        var lp = pendingResolve;\n",
        "        pendingResolve = null;\n",
        "        lp(result);\n",
        "      }\n",
        "    }\n",
        "    \n",
        "    async function createDom() {\n",
        "      if (div !== null) {\n",
        "        return stream;\n",
        "      }\n",
        "\n",
        "      div = document.createElement('div');\n",
        "      div.style.border = '2px solid black';\n",
        "      div.style.padding = '3px';\n",
        "      div.style.width = '100%';\n",
        "      div.style.maxWidth = '600px';\n",
        "      document.body.appendChild(div);\n",
        "      \n",
        "      const modelOut = document.createElement('div');\n",
        "      modelOut.innerHTML = \"<span>Status:</span>\";\n",
        "      labelElement = document.createElement('span');\n",
        "      labelElement.innerText = 'No data';\n",
        "      labelElement.style.fontWeight = 'bold';\n",
        "      modelOut.appendChild(labelElement);\n",
        "      div.appendChild(modelOut);\n",
        "           \n",
        "      video = document.createElement('video');\n",
        "      video.style.display = 'block';\n",
        "      video.width = div.clientWidth - 6;\n",
        "      video.setAttribute('playsinline', '');\n",
        "      video.onclick = () => { shutdown = true; };\n",
        "      stream = await navigator.mediaDevices.getUserMedia(\n",
        "          {video: { facingMode: \"environment\"}});\n",
        "      div.appendChild(video);\n",
        "\n",
        "      imgElement = document.createElement('img');\n",
        "      imgElement.style.position = 'absolute';\n",
        "      imgElement.style.zIndex = 1;\n",
        "      imgElement.onclick = () => { shutdown = true; };\n",
        "      div.appendChild(imgElement);\n",
        "      \n",
        "      const instruction = document.createElement('div');\n",
        "      instruction.innerHTML = \n",
        "          '<span style=\"color: red; font-weight: bold;\">' +\n",
        "          'When finished, click here or on the video to stop this demo</span>';\n",
        "      div.appendChild(instruction);\n",
        "      instruction.onclick = () => { shutdown = true; };\n",
        "      \n",
        "      video.srcObject = stream;\n",
        "      await video.play();\n",
        "\n",
        "      captureCanvas = document.createElement('canvas');\n",
        "      captureCanvas.width = 640; //video.videoWidth;\n",
        "      captureCanvas.height = 480; //video.videoHeight;\n",
        "      window.requestAnimationFrame(onAnimationFrame);\n",
        "      \n",
        "      return stream;\n",
        "    }\n",
        "    async function stream_frame(label, imgData) {\n",
        "      if (shutdown) {\n",
        "        removeDom();\n",
        "        shutdown = false;\n",
        "        return '';\n",
        "      }\n",
        "\n",
        "      var preCreate = Date.now();\n",
        "      stream = await createDom();\n",
        "      \n",
        "      var preShow = Date.now();\n",
        "      if (label != \"\") {\n",
        "        labelElement.innerHTML = label;\n",
        "      }\n",
        "            \n",
        "      if (imgData != \"\") {\n",
        "        var videoRect = video.getClientRects()[0];\n",
        "        imgElement.style.top = videoRect.top + \"px\";\n",
        "        imgElement.style.left = videoRect.left + \"px\";\n",
        "        imgElement.style.width = videoRect.width + \"px\";\n",
        "        imgElement.style.height = videoRect.height + \"px\";\n",
        "        imgElement.src = imgData;\n",
        "      }\n",
        "      \n",
        "      var preCapture = Date.now();\n",
        "      var result = await new Promise(function(resolve, reject) {\n",
        "        pendingResolve = resolve;\n",
        "      });\n",
        "      shutdown = false;\n",
        "      \n",
        "      return {'create': preShow - preCreate, \n",
        "              'show': preCapture - preShow, \n",
        "              'capture': Date.now() - preCapture,\n",
        "              'img': result};\n",
        "    }\n",
        "    ''')\n",
        "\n",
        "  display(js)\n",
        "  \n",
        "def video_frame(label, bbox):\n",
        "  data = eval_js('stream_frame(\"{}\", \"{}\")'.format(label, bbox))\n",
        "  return data"
      ],
      "metadata": {
        "id": "xolSF-IvXju5"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image, ImageDraw\n",
        "\n",
        "# start streaming video from webcam\n",
        "video_stream()\n",
        "# label for video\n",
        "label_html = 'Capturing...'\n",
        "# initialze bounding box to empty\n",
        "bbox = ''\n",
        "count = 0 \n",
        "face_manager = FaceLandmarkManager()\n",
        "\n",
        "while True:\n",
        "    js_reply = video_frame(label_html, bbox)\n",
        "    if not js_reply:\n",
        "        break\n",
        "\n",
        "    # convert JS response to OpenCV Image\n",
        "    img = js_to_image(js_reply[\"img\"])\n",
        "\n",
        "    # create transparent overlay for bounding box\n",
        "    bbox_array = np.zeros([480,640,4], dtype=np.uint8)\n",
        "\n",
        "    # grayscale image for face detection\n",
        "    gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
        "    decimg = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    # 顔のランドマークリストを取得\n",
        "    face_manager.clear_face_landmark_list()\n",
        "    face_manager.detect_face_landmark(decimg)\n",
        "    face_landmark_list = face_manager.get_face_landmark_list()\n",
        "\n",
        "    for face in face_manager._faces:\n",
        "        (x, y, w, h) = face_utils.rect_to_bb(face)\n",
        "        #print(\"face_bbox:\",x,y,w,h)\n",
        "    if 0:\n",
        "        bbox_array = cv2.rectangle(bbox_array,(x,y),(x+w,y+h),(255,0,0),2)\n",
        "\n",
        "    # 目領域の取得\n",
        "    for face_landmark in face_landmark_list:\n",
        "\n",
        "        eye_manager = EyeSystemManager()\n",
        "        eye_manager.detect_eye_region(face_landmark)\n",
        "\n",
        "        # 虹彩領域の取得\n",
        "        right_iris, left_iris = eye_manager.detect_iris_info(decimg)\n",
        "\n",
        "        # 虹彩領域の描画\n",
        "        imgL = cv2.circle(bbox_array, right_iris['center'], right_iris['radius'], (0, 255, 0), 1)\n",
        "        imgR = cv2.circle(bbox_array, left_iris['center'], left_iris['radius'], (0, 255, 0), 1)\n",
        "    if 1:\n",
        "      bbox_array[:,:,3] = (bbox_array.max(axis = 2) > 0 ).astype(int) * 255\n",
        "      bbox_bytes = bbox_to_bytes(bbox_array)\n",
        "      bbox = bbox_bytes\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 785
        },
        "id": "9EmVcy3FXoD-",
        "outputId": "58d57f06-2908-4475-9db2-f7efed69599e"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    var video;\n",
              "    var div = null;\n",
              "    var stream;\n",
              "    var captureCanvas;\n",
              "    var imgElement;\n",
              "    var labelElement;\n",
              "    \n",
              "    var pendingResolve = null;\n",
              "    var shutdown = false;\n",
              "    \n",
              "    function removeDom() {\n",
              "       stream.getVideoTracks()[0].stop();\n",
              "       video.remove();\n",
              "       div.remove();\n",
              "       video = null;\n",
              "       div = null;\n",
              "       stream = null;\n",
              "       imgElement = null;\n",
              "       captureCanvas = null;\n",
              "       labelElement = null;\n",
              "    }\n",
              "    \n",
              "    function onAnimationFrame() {\n",
              "      if (!shutdown) {\n",
              "        window.requestAnimationFrame(onAnimationFrame);\n",
              "      }\n",
              "      if (pendingResolve) {\n",
              "        var result = \"\";\n",
              "        if (!shutdown) {\n",
              "          captureCanvas.getContext('2d').drawImage(video, 0, 0, 640, 480);\n",
              "          result = captureCanvas.toDataURL('image/jpeg', 0.8)\n",
              "        }\n",
              "        var lp = pendingResolve;\n",
              "        pendingResolve = null;\n",
              "        lp(result);\n",
              "      }\n",
              "    }\n",
              "    \n",
              "    async function createDom() {\n",
              "      if (div !== null) {\n",
              "        return stream;\n",
              "      }\n",
              "\n",
              "      div = document.createElement('div');\n",
              "      div.style.border = '2px solid black';\n",
              "      div.style.padding = '3px';\n",
              "      div.style.width = '100%';\n",
              "      div.style.maxWidth = '600px';\n",
              "      document.body.appendChild(div);\n",
              "      \n",
              "      const modelOut = document.createElement('div');\n",
              "      modelOut.innerHTML = \"<span>Status:</span>\";\n",
              "      labelElement = document.createElement('span');\n",
              "      labelElement.innerText = 'No data';\n",
              "      labelElement.style.fontWeight = 'bold';\n",
              "      modelOut.appendChild(labelElement);\n",
              "      div.appendChild(modelOut);\n",
              "           \n",
              "      video = document.createElement('video');\n",
              "      video.style.display = 'block';\n",
              "      video.width = div.clientWidth - 6;\n",
              "      video.setAttribute('playsinline', '');\n",
              "      video.onclick = () => { shutdown = true; };\n",
              "      stream = await navigator.mediaDevices.getUserMedia(\n",
              "          {video: { facingMode: \"environment\"}});\n",
              "      div.appendChild(video);\n",
              "\n",
              "      imgElement = document.createElement('img');\n",
              "      imgElement.style.position = 'absolute';\n",
              "      imgElement.style.zIndex = 1;\n",
              "      imgElement.onclick = () => { shutdown = true; };\n",
              "      div.appendChild(imgElement);\n",
              "      \n",
              "      const instruction = document.createElement('div');\n",
              "      instruction.innerHTML = \n",
              "          '<span style=\"color: red; font-weight: bold;\">' +\n",
              "          'When finished, click here or on the video to stop this demo</span>';\n",
              "      div.appendChild(instruction);\n",
              "      instruction.onclick = () => { shutdown = true; };\n",
              "      \n",
              "      video.srcObject = stream;\n",
              "      await video.play();\n",
              "\n",
              "      captureCanvas = document.createElement('canvas');\n",
              "      captureCanvas.width = 640; //video.videoWidth;\n",
              "      captureCanvas.height = 480; //video.videoHeight;\n",
              "      window.requestAnimationFrame(onAnimationFrame);\n",
              "      \n",
              "      return stream;\n",
              "    }\n",
              "    async function stream_frame(label, imgData) {\n",
              "      if (shutdown) {\n",
              "        removeDom();\n",
              "        shutdown = false;\n",
              "        return '';\n",
              "      }\n",
              "\n",
              "      var preCreate = Date.now();\n",
              "      stream = await createDom();\n",
              "      \n",
              "      var preShow = Date.now();\n",
              "      if (label != \"\") {\n",
              "        labelElement.innerHTML = label;\n",
              "      }\n",
              "            \n",
              "      if (imgData != \"\") {\n",
              "        var videoRect = video.getClientRects()[0];\n",
              "        imgElement.style.top = videoRect.top + \"px\";\n",
              "        imgElement.style.left = videoRect.left + \"px\";\n",
              "        imgElement.style.width = videoRect.width + \"px\";\n",
              "        imgElement.style.height = videoRect.height + \"px\";\n",
              "        imgElement.src = imgData;\n",
              "      }\n",
              "      \n",
              "      var preCapture = Date.now();\n",
              "      var result = await new Promise(function(resolve, reject) {\n",
              "        pendingResolve = resolve;\n",
              "      });\n",
              "      shutdown = false;\n",
              "      \n",
              "      return {'create': preShow - preCreate, \n",
              "              'show': preCapture - preShow, \n",
              "              'capture': Date.now() - preCapture,\n",
              "              'img': result};\n",
              "    }\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-a503a8b244e8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mjs_reply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvideo_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel_html\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbbox\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mjs_reply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-a05efe86d862>\u001b[0m in \u001b[0;36mvideo_frame\u001b[0;34m(label, bbox)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mvideo_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbbox\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m   \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_js\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'stream_frame(\"{}\", \"{}\")'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbbox\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/output/_js.py\u001b[0m in \u001b[0;36meval_js\u001b[0;34m(script, ignore_result, timeout_sec)\u001b[0m\n\u001b[1;32m     38\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mignore_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_message\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_next_input_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_NOT_READY\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m       \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.025\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m       \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     if (reply.get('type') == 'colab_reply' and\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "W3qnjaIeHs0m"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}